{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ffaaeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import json\n",
    "# Get cuurent robot position\n",
    "current_pos = 20\n",
    "point_cloud = o3d.io.read_point_cloud(f'track_1/{current_pos}.ply')\n",
    "\n",
    "#visualize the point cloud\n",
    "# o3d.visualization.draw_geometries([point_cloud], window_name=\"Point Cloud Viewer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7a9c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.73133278 1.71448755 0.02601008]\n",
      "{'speed': 1.151845097541809, 'gear': 1, 'rpm': 499.7661437988281, 'maxrpm': 500.0, 'handbrake': False, 'kinematics_estimated': {'position': {'x_val': 6.731332778930664, 'y_val': 1.7144875526428223, 'z_val': 0.026010077446699142}, 'orientation': {'x_val': 2.454419700370636e-05, 'y_val': -8.552803046768531e-05, 'z_val': 0.27586638927459717, 'w_val': 0.9611960053443909}, 'linear_velocity': {'x_val': 0.9764055609703064, 'y_val': 0.6110478639602661, 'z_val': -2.108308399328962e-05}, 'angular_velocity': {'x_val': 6.667548859695671e-07, 'y_val': -0.0038102702237665653, 'z_val': -0.00044463755330070853}, 'linear_acceleration': {'x_val': -0.014018667861819267, 'y_val': -0.008775300346314907, 'z_val': -0.0018883704906329513}, 'angular_acceleration': {'x_val': 9.44349158089608e-05, 'y_val': -0.2650919556617737, 'z_val': -0.00972615834325552}}, 'timestamp': 1746108922845651500}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "with open(f'track_1/{current_pos}_car_state.json', 'r') as f:\n",
    "    car_1 = json.load(f)\n",
    "with open(f'track_1/{current_pos+15}_car_state.json', 'r') as ft:\n",
    "    car_2 = json.load(ft)\n",
    "\n",
    "car_pos = np.array([car_2['kinematics_estimated']['position']['x_val'], car_2['kinematics_estimated']['position']['y_val'], car_2['kinematics_estimated']['position']['z_val']])\n",
    "print(car_pos)\n",
    "print(car_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed6fd3f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.56337047e+00  2.73348377e-07 -5.69020331e-01]\n",
      " [ 1.56337047e+00  2.73348405e-07 -5.49478948e-01]\n",
      " [ 1.68841732e+00  2.95212288e-07 -5.72488487e-01]\n",
      " ...\n",
      " [ 2.80506873e+00 -1.38890162e-01 -5.83967030e-01]\n",
      " [ 2.80506825e+00 -1.38890132e-01 -5.51572382e-01]\n",
      " [ 2.80506873e+00 -1.38890147e-01 -5.19318581e-01]]\n",
      "{'position': {'x_val': 4.105386257171631, 'y_val': -0.25966617465019226, 'z_val': 0.026110583916306496}, 'orientation': {'x_val': 9.580528967489954e-06, 'y_val': -5.2504106861306354e-05, 'z_val': 0.18658044934272766, 'w_val': 0.9824396967887878}, 'linear_velocity': {'x_val': 1.1782411336898804, 'y_val': 0.4667007029056549, 'z_val': -1.2123963642807212e-05}, 'angular_velocity': {'x_val': 4.755701593239792e-05, 'y_val': -0.0015654905000701547, 'z_val': 0.5049094557762146}, 'linear_acceleration': {'x_val': -0.21480172872543335, 'y_val': 0.5163766145706177, 'z_val': -0.0007224251748993993}, 'angular_acceleration': {'x_val': 0.0006045398768037558, 'y_val': -0.09282144904136658, 'z_val': 8.68217658996582}}\n",
      "[[ 5.55996435  0.31350018 -0.54274287]\n",
      " [ 5.55996241  0.31349943 -0.52320149]\n",
      " [ 5.67630521  0.35934352 -0.54619768]\n",
      " ...\n",
      " [ 6.7661298   0.63949712 -0.55755693]\n",
      " [ 6.76612612  0.63949573 -0.52516228]\n",
      " [ 6.76612336  0.63949465 -0.49290848]]\n",
      "[6.73133278 1.71448755 0.02601008]\n"
     ]
    }
   ],
   "source": [
    "# get convert the point cloud to global coordinates\n",
    "lidar_points = np.asarray(point_cloud.points)\n",
    "print(lidar_points)\n",
    "def quaternion_to_rotation_matrix(q):\n",
    "        qw, qx, qy, qz = q['w_val'], q['x_val'], q['y_val'], q['z_val']\n",
    "        return np.array([\n",
    "            [1 - 2*qy*qy - 2*qz*qz, 2*qx*qy - 2*qz*qw, 2*qx*qz + 2*qy*qw],\n",
    "            [2*qx*qy + 2*qz*qw, 1 - 2*qx*qx - 2*qz*qz, 2*qy*qz - 2*qx*qw],\n",
    "            [2*qx*qz - 2*qy*qw, 2*qy*qz + 2*qx*qw, 1 - 2*qx*qx - 2*qy*qy]\n",
    "        ])\n",
    "def get_vehicle_pose(car_dict):\n",
    "    vehicle_pose = car_dict['kinematics_estimated']\n",
    "    print(vehicle_pose)\n",
    "    pos = vehicle_pose['position']\n",
    "    orient = vehicle_pose['orientation']\n",
    "    position_array = np.array([float(pos['x_val']), float(pos['y_val']), float(pos['z_val'])])\n",
    "    rotation_matrix = quaternion_to_rotation_matrix(orient)\n",
    "    return position_array, rotation_matrix\n",
    "def transform_to_world(points, position, rotation_matrix):\n",
    "    points_rotated = np.dot(points, rotation_matrix.T)\n",
    "    return points_rotated + position\n",
    "position_1, rotation_matrix_1 = get_vehicle_pose(car_1)\n",
    "# get the world coordinates of the points\n",
    "points_world_1 = transform_to_world(lidar_points, position_1, rotation_matrix_1)\n",
    "print(points_world_1)\n",
    "print(car_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23b4884d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.674492208748337 6.957939372657722\n"
     ]
    }
   ],
   "source": [
    "# label the points as 0.5\n",
    "labels = np.zeros(points_world_1.shape[0]) + 0.5\n",
    "\n",
    "# then mark the points that are closed to the car as 1.0\n",
    "distances = np.linalg.norm(points_world_1 - car_pos, axis=1)\n",
    "print(min(distances), max(distances))\n",
    "threshold = 1.2\n",
    "\n",
    "labels[distances < threshold] = 0.0\n",
    "# create a new point cloud with the labels\n",
    "point_cloud_with_labels = o3d.geometry.PointCloud()\n",
    "point_cloud_with_labels.points = o3d.utility.Vector3dVector(lidar_points)\n",
    "# label the points with the labels that 0.5 as orange and 0.0 as green\n",
    "colors = np.zeros((len(labels), 3))\n",
    "colors[labels == 0.5] = [1, 0.5, 0]  # orange\n",
    "colors[labels == 0.0] = [0, 1, 0]  # green\n",
    "point_cloud_with_labels.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "# visualize the point cloud with labels\n",
    "# o3d.visualization.draw_geometries([point_cloud_with_labels], window_name=\"Point Cloud Viewer with Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3c89c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add labels to the point cloud\n",
    "# point_cloud.colors = o3d.utility.Vector3dVector(np.array([[1, 0, 0]] * len(t)))\n",
    "# visualize the point cloud with labels\n",
    "# o3d.visualization.draw_geometries([point_cloud], window_name=\"Point Cloud Viewer with Labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91c34c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Y: 2.4322943687438965\n",
      "Min Y: -3.195559024810791\n"
     ]
    }
   ],
   "source": [
    "# get the max and min coordinates of the point cloud on y\n",
    "max_y = point_cloud.get_max_bound()[1]\n",
    "min_y = point_cloud.get_min_bound()[1]\n",
    "print(\"Max Y:\", max_y)\n",
    "print(\"Min Y:\", min_y)\n",
    "# get the number of points in the point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3e34c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6853307485580444"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the json\n",
    "import json\n",
    "with open('track_1/1_car_state.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "data['speed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a979ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1: marked 0 additional points as 0.0\n",
      "frame 2: marked 0 additional points as 0.0\n",
      "frame 3: marked 0 additional points as 0.0\n",
      "frame 4: marked 0 additional points as 0.0\n",
      "frame 5: marked 0 additional points as 0.0\n",
      "frame 6: marked 0 additional points as 0.0\n",
      "frame 7: marked 0 additional points as 0.0\n",
      "frame 8: marked 0 additional points as 0.0\n",
      "frame 9: marked 0 additional points as 0.0\n",
      "frame 10: marked 0 additional points as 0.0\n",
      "frame 11: marked 0 additional points as 0.0\n",
      "frame 12: marked 0 additional points as 0.0\n",
      "frame 13: marked 0 additional points as 0.0\n",
      "frame 14: marked 0 additional points as 0.0\n",
      "frame 15: marked 0 additional points as 0.0\n",
      "frame 16: marked 0 additional points as 0.0\n",
      "frame 17: marked 0 additional points as 0.0\n",
      "frame 18: marked 14 additional points as 0.0\n",
      "frame 19: marked 12 additional points as 0.0\n",
      "frame 20: marked 28 additional points as 0.0\n",
      "frame 21: marked 38 additional points as 0.0\n",
      "frame 22: marked 2 additional points as 0.0\n",
      "frame 23: marked 4 additional points as 0.0\n",
      "frame 24: marked 40 additional points as 0.0\n",
      "frame 25: marked 69 additional points as 0.0\n",
      "frame 26: marked 88 additional points as 0.0\n",
      "frame 27: marked 27 additional points as 0.0\n",
      "frame 28: marked 24 additional points as 0.0\n",
      "frame 29: marked 19 additional points as 0.0\n",
      "frame 30: marked 8 additional points as 0.0\n",
      "frame 31: marked 4 additional points as 0.0\n",
      "frame 32: marked 5 additional points as 0.0\n",
      "frame 33: marked 1 additional points as 0.0\n",
      "frame 34: marked 1 additional points as 0.0\n",
      "frame 35: marked 243 additional points as 0.0\n",
      "frame 36: marked 688 additional points as 0.0\n",
      "frame 37: marked 136 additional points as 0.0\n",
      "frame 38: marked 36 additional points as 0.0\n",
      "frame 39: marked 6 additional points as 0.0\n",
      "frame 40: marked 0 additional points as 0.0\n",
      "frame 41: marked 0 additional points as 0.0\n",
      "frame 42: marked 0 additional points as 0.0\n",
      "frame 43: marked 0 additional points as 0.0\n",
      "frame 44: marked 0 additional points as 0.0\n",
      "frame 45: marked 0 additional points as 0.0\n",
      "frame 46: marked 0 additional points as 0.0\n",
      "frame 47: marked 0 additional points as 0.0\n",
      "frame 48: marked 0 additional points as 0.0\n",
      "frame 49: marked 0 additional points as 0.0\n",
      "frame 50: marked 0 additional points as 0.0\n",
      "frame 51: marked 0 additional points as 0.0\n",
      "frame 52: marked 58 additional points as 0.0\n",
      "frame 53: marked 113 additional points as 0.0\n",
      "frame 54: marked 59 additional points as 0.0\n",
      "frame 55: marked 49 additional points as 0.0\n",
      "frame 56: marked 40 additional points as 0.0\n",
      "frame 57: marked 33 additional points as 0.0\n",
      "frame 58: marked 29 additional points as 0.0\n",
      "frame 59: marked 20 additional points as 0.0\n",
      "frame 60: marked 15 additional points as 0.0\n",
      "frame 61: marked 9 additional points as 0.0\n",
      "frame 62: marked 3 additional points as 0.0\n",
      "frame 63: marked 5 additional points as 0.0\n",
      "frame 64: marked 1 additional points as 0.0\n",
      "frame 65: marked 3 additional points as 0.0\n",
      "frame 66: marked 2 additional points as 0.0\n",
      "frame 67: marked 1 additional points as 0.0\n",
      "frame 68: marked 0 additional points as 0.0\n",
      "frame 69: marked 0 additional points as 0.0\n",
      "frame 70: marked 0 additional points as 0.0\n",
      "frame 71: marked 0 additional points as 0.0\n",
      "frame 72: marked 0 additional points as 0.0\n",
      "frame 73: marked 0 additional points as 0.0\n",
      "frame 74: marked 0 additional points as 0.0\n",
      "frame 75: marked 0 additional points as 0.0\n",
      "frame 76: marked 0 additional points as 0.0\n",
      "frame 77: marked 0 additional points as 0.0\n",
      "frame 78: marked 0 additional points as 0.0\n",
      "frame 79: marked 0 additional points as 0.0\n",
      "frame 80: marked 0 additional points as 0.0\n",
      "frame 81: marked 0 additional points as 0.0\n",
      "frame 82: marked 0 additional points as 0.0\n",
      "frame 83: marked 0 additional points as 0.0\n",
      "frame 84: marked 0 additional points as 0.0\n",
      "frame 85: marked 0 additional points as 0.0\n",
      "frame 86: marked 0 additional points as 0.0\n",
      "frame 87: marked 0 additional points as 0.0\n",
      "frame 88: marked 0 additional points as 0.0\n",
      "frame 89: marked 0 additional points as 0.0\n",
      "frame 90: marked 0 additional points as 0.0\n",
      "frame 91: marked 0 additional points as 0.0\n",
      "frame 92: marked 0 additional points as 0.0\n",
      "frame 93: marked 0 additional points as 0.0\n",
      "frame 94: marked 0 additional points as 0.0\n",
      "[STOP] no more car_state at frame 95\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# parameters\n",
    "current_pos = 50\n",
    "threshold = 1.2\n",
    "\n",
    "# 1) load & transform the LiDAR once\n",
    "pc = o3d.io.read_point_cloud(f'track_1/{current_pos}.ply')\n",
    "lidar_points = np.asarray(pc.points)\n",
    "\n",
    "with open(f'track_1/{current_pos}_car_state.json', 'r') as f:\n",
    "    car_0 = json.load(f)\n",
    "\n",
    "def quaternion_to_rotation_matrix(q):\n",
    "    qw, qx, qy, qz = q['w_val'], q['x_val'], q['y_val'], q['z_val']\n",
    "    return np.array([\n",
    "        [1 - 2*qy*qy - 2*qz*qz,  2*qx*qy - 2*qz*qw,    2*qx*qz + 2*qy*qw],\n",
    "        [2*qx*qy + 2*qz*qw,      1 - 2*qx*qx - 2*qz*qz,2*qy*qz - 2*qx*qw],\n",
    "        [2*qx*qz - 2*qy*qw,      2*qy*qz + 2*qx*qw,    1 - 2*qx*qx - 2*qy*qy]\n",
    "    ])\n",
    "\n",
    "def get_vehicle_pose(car_dict):\n",
    "    pos = car_dict['kinematics_estimated']['position']\n",
    "    ori = car_dict['kinematics_estimated']['orientation']\n",
    "    position = np.array([pos['x_val'], pos['y_val'], pos['z_val']], dtype=float)\n",
    "    R = quaternion_to_rotation_matrix(ori)\n",
    "    return position, R\n",
    "\n",
    "pos0, R0 = get_vehicle_pose(car_0)\n",
    "# world‐coords of each LiDAR point\n",
    "points_world = (R0 @ lidar_points.T).T + pos0\n",
    "\n",
    "# 2) initialize all labels to 0.5 (orange)\n",
    "labels = np.full(len(points_world), 0.5, dtype=float)\n",
    "\n",
    "# 3) loop forward until no points fall within the threshold\n",
    "i = 1\n",
    "while True:\n",
    "    idx = i\n",
    "    try:\n",
    "        with open(f'track_1/{idx}_car_state.json', 'r') as f:\n",
    "            car_i = json.load(f)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"[STOP] no more car_state at frame {idx}\")\n",
    "        break\n",
    "\n",
    "    pos_i, _ = get_vehicle_pose(car_i)\n",
    "    dists = np.linalg.norm(points_world - pos_i, axis=1)\n",
    "\n",
    "    close_mask = dists < threshold\n",
    "    num_new = np.count_nonzero(close_mask & (labels != 0.0))\n",
    "    # if num_new == 0:\n",
    "    #     print(f\"[DONE] at frame {idx}, no new points within {threshold} m.\")\n",
    "    #     break\n",
    "\n",
    "    # **only** mark those still‐unmarked points that are now too close\n",
    "    labels[close_mask] = 0.0\n",
    "    print(f\"frame {idx}: marked {num_new} additional points as 0.0\")\n",
    "\n",
    "    i += 1\n",
    "\n",
    "# 4) build a colored point‐cloud once, preserving all 0.0 and 0.5 labels\n",
    "colors = np.zeros((len(labels), 3))\n",
    "colors[labels == 0.5] = [1, 0.5, 0]  # orange\n",
    "colors[labels == 0.0] = [0, 1, 0]    # green\n",
    "\n",
    "pc_labeled = o3d.geometry.PointCloud()\n",
    "pc_labeled.points = o3d.utility.Vector3dVector(lidar_points)\n",
    "pc_labeled.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "# 5) visualize\n",
    "o3d.visualization.draw_geometries(\n",
    "    [pc_labeled],\n",
    "    window_name=\"Sticky‐Labels Point Cloud\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e889f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1: +0 newly < 1.2 m\n",
      "frame 2: +0 newly < 1.2 m\n",
      "frame 3: +0 newly < 1.2 m\n",
      "frame 4: +0 newly < 1.2 m\n",
      "frame 5: +0 newly < 1.2 m\n",
      "frame 6: +0 newly < 1.2 m\n",
      "frame 7: +0 newly < 1.2 m\n",
      "frame 8: +0 newly < 1.2 m\n",
      "frame 9: +0 newly < 1.2 m\n",
      "frame 10: +0 newly < 1.2 m\n",
      "frame 11: +0 newly < 1.2 m\n",
      "frame 12: +0 newly < 1.2 m\n",
      "frame 13: +0 newly < 1.2 m\n",
      "frame 14: +0 newly < 1.2 m\n",
      "frame 15: +0 newly < 1.2 m\n",
      "frame 16: +0 newly < 1.2 m\n",
      "frame 17: +0 newly < 1.2 m\n",
      "frame 18: +1 newly < 1.2 m\n",
      "frame 19: +4 newly < 1.2 m\n",
      "frame 20: +24 newly < 1.2 m\n",
      "frame 21: +17 newly < 1.2 m\n",
      "frame 22: +4 newly < 1.2 m\n",
      "frame 23: +2 newly < 1.2 m\n",
      "frame 24: +23 newly < 1.2 m\n",
      "frame 25: +62 newly < 1.2 m\n",
      "frame 26: +103 newly < 1.2 m\n",
      "frame 27: +96 newly < 1.2 m\n",
      "frame 28: +43 newly < 1.2 m\n",
      "frame 29: +28 newly < 1.2 m\n",
      "frame 30: +15 newly < 1.2 m\n",
      "frame 31: +7 newly < 1.2 m\n",
      "frame 32: +4 newly < 1.2 m\n",
      "frame 33: +0 newly < 1.2 m\n",
      "frame 34: +0 newly < 1.2 m\n",
      "frame 35: +728 newly < 1.2 m\n",
      "frame 36: +134 newly < 1.2 m\n",
      "frame 37: +63 newly < 1.2 m\n",
      "frame 38: +0 newly < 1.2 m\n",
      "frame 39: +0 newly < 1.2 m\n",
      "frame 40: +0 newly < 1.2 m\n",
      "frame 41: +0 newly < 1.2 m\n",
      "frame 42: +0 newly < 1.2 m\n",
      "frame 43: +0 newly < 1.2 m\n",
      "frame 44: +0 newly < 1.2 m\n",
      "frame 45: +0 newly < 1.2 m\n",
      "frame 46: +4 newly < 1.2 m\n",
      "frame 47: +0 newly < 1.2 m\n",
      "frame 48: +0 newly < 1.2 m\n",
      "frame 49: +0 newly < 1.2 m\n",
      "frame 50: +1 newly < 1.2 m\n",
      "frame 51: +6 newly < 1.2 m\n",
      "frame 52: +32 newly < 1.2 m\n",
      "frame 53: +46 newly < 1.2 m\n",
      "frame 54: +67 newly < 1.2 m\n",
      "frame 55: +41 newly < 1.2 m\n",
      "frame 56: +32 newly < 1.2 m\n",
      "frame 57: +35 newly < 1.2 m\n",
      "frame 58: +25 newly < 1.2 m\n",
      "frame 59: +16 newly < 1.2 m\n",
      "frame 60: +5 newly < 1.2 m\n",
      "frame 61: +12 newly < 1.2 m\n",
      "frame 62: +2 newly < 1.2 m\n",
      "frame 63: +4 newly < 1.2 m\n",
      "frame 64: +1 newly < 1.2 m\n",
      "frame 65: +2 newly < 1.2 m\n",
      "frame 66: +0 newly < 1.2 m\n",
      "frame 67: +0 newly < 1.2 m\n",
      "frame 68: +2 newly < 1.2 m\n",
      "frame 69: +0 newly < 1.2 m\n",
      "frame 70: +0 newly < 1.2 m\n",
      "frame 71: +0 newly < 1.2 m\n",
      "frame 72: +0 newly < 1.2 m\n",
      "frame 73: +0 newly < 1.2 m\n",
      "frame 74: +0 newly < 1.2 m\n",
      "frame 75: +0 newly < 1.2 m\n",
      "frame 76: +0 newly < 1.2 m\n",
      "frame 77: +0 newly < 1.2 m\n",
      "frame 78: +0 newly < 1.2 m\n",
      "frame 79: +0 newly < 1.2 m\n",
      "frame 80: +0 newly < 1.2 m\n",
      "frame 81: +0 newly < 1.2 m\n",
      "frame 82: +0 newly < 1.2 m\n",
      "frame 83: +0 newly < 1.2 m\n",
      "frame 84: +0 newly < 1.2 m\n",
      "frame 85: +0 newly < 1.2 m\n",
      "frame 86: +0 newly < 1.2 m\n",
      "frame 87: +0 newly < 1.2 m\n",
      "frame 88: +0 newly < 1.2 m\n",
      "frame 89: +0 newly < 1.2 m\n",
      "frame 90: +0 newly < 1.2 m\n",
      "frame 91: +0 newly < 1.2 m\n",
      "frame 92: +0 newly < 1.2 m\n",
      "frame 93: +0 newly < 1.2 m\n",
      "frame 94: +0 newly < 1.2 m\n",
      "[stop] no more car_state at frame 95\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# params\n",
    "current_pos = 38\n",
    "threshold   = 1.2\n",
    "\n",
    "# 1) load & transform your LiDAR just once\n",
    "pc = o3d.io.read_point_cloud(f'track_1/{current_pos}.ply')\n",
    "lidar_pts = np.asarray(pc.points)\n",
    "\n",
    "with open(f'track_1/{current_pos}_car_state.json','r') as f:\n",
    "    car0 = json.load(f)\n",
    "\n",
    "def quat2rot(q):\n",
    "    qw, qx, qy, qz = q['w_val'], q['x_val'], q['y_val'], q['z_val']\n",
    "    return np.array([\n",
    "        [1-2*qy*qy-2*qz*qz, 2*qx*qy-2*qz*qw,   2*qx*qz+2*qy*qw],\n",
    "        [2*qx*qy+2*qz*qw,   1-2*qx*qx-2*qz*qz, 2*qy*qz-2*qx*qw],\n",
    "        [2*qx*qz-2*qy*qw,   2*qy*qz+2*qx*qw,   1-2*qx*qx-2*qy*qy]\n",
    "    ])\n",
    "\n",
    "def get_pose(state):\n",
    "    p = state['kinematics_estimated']['position']\n",
    "    o = state['kinematics_estimated']['orientation']\n",
    "    pos = np.array([p['x_val'], p['y_val'], p['z_val']], dtype=float)\n",
    "    R   = quat2rot(o)\n",
    "    return pos, R\n",
    "\n",
    "pos0, R0 = get_pose(car0)\n",
    "# world-coords of LiDAR\n",
    "world_pts = (R0 @ lidar_pts.T).T + pos0\n",
    "\n",
    "# 2) prepare an “ever-close” boolean mask\n",
    "ever_close = np.zeros(len(world_pts), dtype=bool)\n",
    "\n",
    "# 3) sweep forward until no frame introduces new “close” points\n",
    "i = 1\n",
    "while True:\n",
    "    idx =  i\n",
    "    try:\n",
    "        with open(f'track_1/{idx}_car_state.json','r') as f:\n",
    "            car_i = json.load(f)\n",
    "        with open(f'track_1/{idx}_collisioin_info.json','r') as f:\n",
    "            collision_info = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[stop] no more car_state at frame {idx}\")\n",
    "        break\n",
    "\n",
    "    pos_i, _    = get_pose(car_i)\n",
    "    dists       = np.linalg.norm(world_pts - pos_i, axis=1)\n",
    "    close_mask  = dists < threshold\n",
    "\n",
    "    # record any new close points\n",
    "    new_hits = close_mask & ~ever_close\n",
    "    ever_close |= close_mask\n",
    "\n",
    "\n",
    "    print(f\"frame {idx}: +{new_hits.sum()} newly < {threshold} m\")\n",
    "    i += 1\n",
    "\n",
    "# 4) color the point cloud: gray for “never close”, green for “ever close”\n",
    "colors = np.ones((len(world_pts), 3)) * 0.7   # default light gray\n",
    "colors[ever_close] = [0, 1, 0]                # green\n",
    "\n",
    "pc.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "# 5) visualize\n",
    "o3d.visualization.draw_geometries([pc], window_name=\"Only‐Zero Labeling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818d2981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame 1: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 2: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 3: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 4: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 5: +27 new car‐hits, +0 new collision‐hits\n",
      "frame 6: +51 new car‐hits, +0 new collision‐hits\n",
      "frame 7: +41 new car‐hits, +0 new collision‐hits\n",
      "frame 8: +57 new car‐hits, +0 new collision‐hits\n",
      "frame 9: +67 new car‐hits, +0 new collision‐hits\n",
      "frame 10: +46 new car‐hits, +0 new collision‐hits\n",
      "frame 11: +24 new car‐hits, +0 new collision‐hits\n",
      "frame 12: +30 new car‐hits, +0 new collision‐hits\n",
      "frame 13: +15 new car‐hits, +0 new collision‐hits\n",
      "frame 14: +16 new car‐hits, +0 new collision‐hits\n",
      "frame 15: +5 new car‐hits, +0 new collision‐hits\n",
      "frame 16: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 17: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 18: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 19: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 20: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 21: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 22: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 23: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 24: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 25: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 26: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 27: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 28: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 29: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 30: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 31: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 32: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 33: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 34: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 35: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 36: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 37: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 38: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 39: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 40: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 41: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 42: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 43: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 44: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 45: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 46: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 47: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 48: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 49: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 50: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 51: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 52: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 53: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 54: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 55: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 56: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 57: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 58: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 59: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 60: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 61: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 62: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 63: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 64: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 65: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 66: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 67: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 68: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 69: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 70: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 71: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 72: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 73: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 74: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 75: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 76: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 77: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 78: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 79: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 80: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 81: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 82: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 83: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 84: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 85: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 86: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 87: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 88: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 89: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 90: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 91: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 92: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 93: +0 new car‐hits, +0 new collision‐hits\n",
      "frame 94: +0 new car‐hits, +0 new collision‐hits\n",
      "[stop] no more data at frame 95\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# params\n",
    "current_pos     = 1\n",
    "threshold       = 1.2\n",
    "\n",
    "# 1) load & transform LiDAR once\n",
    "pc             = o3d.io.read_point_cloud(f'track_1/{current_pos}.ply')\n",
    "lidar_pts      = np.asarray(pc.points)\n",
    "with open(f'track_1/{current_pos}_car_state.json','r') as f:\n",
    "    car0        = json.load(f)\n",
    "\n",
    "def quat2rot(q):\n",
    "    qw, qx, qy, qz = q['w_val'], q['x_val'], q['y_val'], q['z_val']\n",
    "    return np.array([\n",
    "        [1-2*qy*qy-2*qz*qz, 2*qx*qy-2*qz*qw,   2*qx*qz+2*qy*qw],\n",
    "        [2*qx*qy+2*qz*qw,   1-2*qx*qx-2*qz*qz, 2*qy*qz-2*qx*qw],\n",
    "        [2*qx*qz-2*qy*qw,   2*qy*qz+2*qx*qw,   1-2*qx*qx-2*qy*qy]\n",
    "    ])\n",
    "\n",
    "def get_pose(state):\n",
    "    p = state['kinematics_estimated']['position']\n",
    "    o = state['kinematics_estimated']['orientation']\n",
    "    pos = np.array([p['x_val'], p['y_val'], p['z_val']], dtype=float)\n",
    "    return pos, quat2rot(o)\n",
    "\n",
    "pos0, R0      = get_pose(car0)\n",
    "world_pts     = (R0 @ lidar_pts.T).T + pos0\n",
    "\n",
    "# 2) two “ever”-masks\n",
    "ever_close     = np.zeros(len(world_pts), dtype=bool)\n",
    "ever_collision = np.zeros(len(world_pts), dtype=bool)\n",
    "\n",
    "# 3) sweep forward through frames\n",
    "i = 1\n",
    "while True:\n",
    "    idx = i\n",
    "    try:\n",
    "        with open(f'track_1/{idx}_car_state.json','r') as f:\n",
    "            car_i = json.load(f)\n",
    "        with open(f'track_1/{idx}_collision_info.json','r') as f:\n",
    "            col_i = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[stop] no more data at frame {idx}\")\n",
    "        break\n",
    "\n",
    "    # car‐proximity\n",
    "    pos_i, _       = get_pose(car_i)\n",
    "    d_car          = np.linalg.norm(world_pts - pos_i, axis=1)\n",
    "    close_mask     = d_car < threshold\n",
    "    new_car_hits   = close_mask & ~ever_close\n",
    "    ever_close    |= close_mask\n",
    "\n",
    "    # collision‐proximity\n",
    "    #  — adapt these keys to your JSON structure —\n",
    "    cp = col_i['position']\n",
    "    col_pos = np.array([cp['x_val'], cp['y_val'], cp['z_val']], dtype=float)\n",
    "    d_col         = np.linalg.norm(world_pts - col_pos, axis=1)\n",
    "    col_mask      = d_col < threshold\n",
    "    new_col_hits  = col_mask & ~ever_collision\n",
    "    ever_collision |= col_mask\n",
    "\n",
    "    print(f\"frame {idx}: +{new_car_hits.sum()} new car‐hits, +{new_col_hits.sum()} new collision‐hits\")\n",
    "    i += 1\n",
    "\n",
    "# 4) assemble a single integer label array\n",
    "#    0 = never-close, 1 = collision, 2 = car-only\n",
    "labels = np.zeros(len(world_pts), dtype=int)\n",
    "labels[ever_close]     = 2\n",
    "labels[ever_collision] = 1\n",
    "\n",
    "# 5) color map for visualization\n",
    "colors = np.ones((len(world_pts), 3)) * 0.7   # default gray\n",
    "colors[labels == 2] = [0, 1, 0]               # green for car\n",
    "colors[labels == 1] = [1, 0, 0]               # red for collision\n",
    "\n",
    "pc.colors = o3d.utility.Vector3dVector(colors)\n",
    "\n",
    "# 6) show\n",
    "o3d.visualization.draw_geometries([pc], window_name=\"Car vs Collision Labeling\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4eaae78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (800, 600) to (800, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] The number of points is 0 when creating axis-aligned bounding box.\n",
      "Added frame 0 to video.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument\n\nFFMPEG COMMAND:\nc:\\Users\\gavin\\.conda\\envs\\lidarAIR\\Lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe -y -f rawvideo -vcodec rawvideo -s 800x600 -pix_fmt rgb24 -r 10.00 -i - -an -vcodec libx264 -pix_fmt yuv420p -vf scale=800:608 -v warning c:\\Users\\gavin\\OneDrive\\Documents\\GitHub\\Cosys-AirSim\\PythonClient\\car\\trying\\test\\output.mp4\n\nFFMPEG STDERR OUTPUT:\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gavin\\.conda\\envs\\lidarAIR\\Lib\\site-packages\\imageio_ffmpeg\\_io.py:627\u001b[39m, in \u001b[36mwrite_frames\u001b[39m\u001b[34m(path, size, pix_fmt_in, pix_fmt_out, fps, quality, bitrate, codec, macro_block_size, ffmpeg_log_level, ffmpeg_timeout, input_params, output_params, audio_path, audio_codec)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     \u001b[43mp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstdin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    629\u001b[39m     \u001b[38;5;66;03m# Show the command and stderr from pipe\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    101\u001b[39m img = np.asarray(vis.capture_screen_float_buffer(do_render=\u001b[38;5;28;01mTrue\u001b[39;00m))   \u001b[38;5;66;03m# RGB floats [0,1]\u001b[39;00m\n\u001b[32m    102\u001b[39m img_uint8 = (img * \u001b[32m255\u001b[39m).astype(np.uint8)                             \u001b[38;5;66;03m# uint8 [0,255]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappend_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_uint8\u001b[49m\u001b[43m)\u001b[49m                                        \u001b[38;5;66;03m# ImageIO expects RGB\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAdded frame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m to video.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    105\u001b[39m time.sleep(\u001b[32m0.1\u001b[39m)  \u001b[38;5;66;03m# optional pause\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gavin\\.conda\\envs\\lidarAIR\\Lib\\site-packages\\imageio\\core\\format.py:590\u001b[39m, in \u001b[36mFormat.Writer.append_data\u001b[39m\u001b[34m(self, im, meta)\u001b[39m\n\u001b[32m    588\u001b[39m im = asarray(im)\n\u001b[32m    589\u001b[39m \u001b[38;5;66;03m# Call\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m590\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_append_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal_meta\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gavin\\.conda\\envs\\lidarAIR\\Lib\\site-packages\\imageio\\plugins\\ffmpeg.py:600\u001b[39m, in \u001b[36mFfmpegFormat.Writer._append_data\u001b[39m\u001b[34m(self, im, meta)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._write_gen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# Check status\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[38;5;66;03m# Write. Yes, we can send the data in as a numpy array\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m600\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_write_gen\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\gavin\\.conda\\envs\\lidarAIR\\Lib\\site-packages\\imageio_ffmpeg\\_io.py:634\u001b[39m, in \u001b[36mwrite_frames\u001b[39m\u001b[34m(path, size, pix_fmt_in, pix_fmt_out, fps, quality, bitrate, codec, macro_block_size, ffmpeg_log_level, ffmpeg_timeout, input_params, output_params, audio_path, audio_codec)\u001b[39m\n\u001b[32m    628\u001b[39m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    629\u001b[39m             \u001b[38;5;66;03m# Show the command and stderr from pipe\u001b[39;00m\n\u001b[32m    630\u001b[39m             msg = (\n\u001b[32m    631\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{0:}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFFMPEG COMMAND:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{1:}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFFMPEG STDERR \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    632\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mOUTPUT:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.format(err, cmd_str)\n\u001b[32m    633\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m634\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(msg)\n\u001b[32m    636\u001b[39m         nframes += \u001b[32m1\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m    639\u001b[39m     \u001b[38;5;66;03m# Note that GeneratorExit does not inherit from Exception but BaseException\u001b[39;00m\n\u001b[32m    640\u001b[39m     \u001b[38;5;66;03m# Detect premature closing\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument\n\nFFMPEG COMMAND:\nc:\\Users\\gavin\\.conda\\envs\\lidarAIR\\Lib\\site-packages\\imageio_ffmpeg\\binaries\\ffmpeg-win-x86_64-v7.1.exe -y -f rawvideo -vcodec rawvideo -s 800x600 -pix_fmt rgb24 -r 10.00 -i - -an -vcodec libx264 -pix_fmt yuv420p -vf scale=800:608 -v warning c:\\Users\\gavin\\OneDrive\\Documents\\GitHub\\Cosys-AirSim\\PythonClient\\car\\trying\\test\\output.mp4\n\nFFMPEG STDERR OUTPUT:\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import imageio\n",
    "\n",
    "# ─── PARAMETERS ────────────────────────────────────────────────────────────────\n",
    "start_frame = 0\n",
    "end_frame   = 93\n",
    "threshold   = 1.2\n",
    "output_path = \"output.mp4\"\n",
    "fps         = 10\n",
    "width, height = 800, 600\n",
    "\n",
    "# ─── HELPERS ──────────────────────────────────────────────────────────────────\n",
    "def quat2rot(q):\n",
    "    qw, qx, qy, qz = q['w_val'], q['x_val'], q['y_val'], q['z_val']\n",
    "    return np.array([\n",
    "        [1-2*qy*qy-2*qz*qz,   2*qx*qy-2*qz*qw,     2*qx*qz+2*qy*qw],\n",
    "        [2*qx*qy+2*qz*qw,     1-2*qx*qx-2*qz*qz,   2*qy*qz-2*qx*qw],\n",
    "        [2*qx*qz-2*qy*qw,     2*qy*qz+2*qx*qw,     1-2*qx*qx-2*qy*qy]\n",
    "    ])\n",
    "\n",
    "def get_pose(state):\n",
    "    p = state['kinematics_estimated']['position']\n",
    "    o = state['kinematics_estimated']['orientation']\n",
    "    pos = np.array([p['x_val'], p['y_val'], p['z_val']], dtype=float)\n",
    "    return pos, quat2rot(o)\n",
    "\n",
    "# ─── SET UP OFFSCREEN VISUALIZER ───────────────────────────────────────────────\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(visible=False, width=width, height=height)\n",
    "pc_vis = o3d.geometry.PointCloud()\n",
    "vis.add_geometry(pc_vis)\n",
    "ctr = vis.get_view_control()\n",
    "ctr.set_front([0.5, -0.5, -1.0])\n",
    "ctr.set_lookat([0, 0, 0])\n",
    "ctr.set_up([0, 0, 1])\n",
    "ctr.set_zoom(0.5)\n",
    "\n",
    "# ─── SET UP IMAGEIO VIDEO WRITER ───────────────────────────────────────────────\n",
    "writer = imageio.get_writer(output_path, fps=fps)\n",
    "\n",
    "# ─── MAIN LOOP ─────────────────────────────────────────────────────────────────\n",
    "for idx in range(start_frame, end_frame + 1):\n",
    "    plyf   = f\"track_1/{idx}.ply\"\n",
    "    carf   = f\"track_1/{idx}_car_state.json\"\n",
    "    collf  = f\"track_1/{idx}_collision_info.json\"\n",
    "    if not (os.path.exists(plyf) and os.path.exists(carf) and os.path.exists(collf)):\n",
    "        print(f\"[skip {idx}] missing base files\")\n",
    "        continue\n",
    "\n",
    "    # load & transform LiDAR into world coords at frame idx\n",
    "    cloud   = o3d.io.read_point_cloud(plyf)\n",
    "    pts     = np.asarray(cloud.points)\n",
    "    car0    = json.load(open(carf))\n",
    "    pos0, R0 = get_pose(car0)\n",
    "    world_pts = (R0 @ pts.T).T + pos0\n",
    "\n",
    "    ever_close     = np.zeros(len(world_pts), dtype=bool)\n",
    "    ever_collision = np.zeros(len(world_pts), dtype=bool)\n",
    "\n",
    "    # sweep through future frames to build masks\n",
    "    j = idx + 1\n",
    "    while True:\n",
    "        car_jf  = f\"track_1/{j}_car_state.json\"\n",
    "        coll_jf = f\"track_1/{j}_collision_info.json\"\n",
    "        if not (os.path.exists(car_jf) and os.path.exists(coll_jf)):\n",
    "            break\n",
    "\n",
    "        car_j  = json.load(open(car_jf))\n",
    "        coll_j = json.load(open(coll_jf))\n",
    "\n",
    "        # proximity mask\n",
    "        pos_car, _ = get_pose(car_j)\n",
    "        d_car      = np.linalg.norm(world_pts - pos_car, axis=1)\n",
    "        ever_close |= (d_car < threshold)\n",
    "\n",
    "        # collision mask\n",
    "        cp        = coll_j['position']\n",
    "        pos_col   = np.array([cp['x_val'], cp['y_val'], cp['z_val']], dtype=float)\n",
    "        d_col     = np.linalg.norm(world_pts - pos_col, axis=1)\n",
    "        ever_collision |= (d_col < threshold)\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # color code: red=ever collided, green=ever close only, gray otherwise\n",
    "    colors = np.ones((len(world_pts), 3)) * 0.7\n",
    "    colors[ever_collision]               = [1, 0, 0]\n",
    "    colors[ever_close & ~ever_collision] = [0, 1, 0]\n",
    "\n",
    "    # update visualizer\n",
    "    pc_vis.points = o3d.utility.Vector3dVector(world_pts)\n",
    "    pc_vis.colors = o3d.utility.Vector3dVector(colors)\n",
    "    vis.update_geometry(pc_vis)\n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "\n",
    "    # capture & append to video\n",
    "    img = np.asarray(vis.capture_screen_float_buffer(do_render=True))   # RGB floats [0,1]\n",
    "    img_uint8 = (img * 255).astype(np.uint8)                             # uint8 [0,255]\n",
    "    writer.append_data(img_uint8)                                        # ImageIO expects RGB\n",
    "    print(f\"Added frame {idx} to video.\")\n",
    "    time.sleep(0.1)  # optional pause\n",
    "\n",
    "# ─── CLEAN UP ─────────────────────────────────────────────────────────────────\n",
    "writer.close()\n",
    "vis.destroy_window()\n",
    "print(f\"Video saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1565016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] The number of points is 0 when creating axis-aligned bounding box.\n",
      "  [stop inner @95] no more future data\n",
      "  [stop inner @95] no more future data\n",
      "  [stop inner @95] no more future data\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     90\u001b[39m pc_vis.colors = o3d.utility.Vector3dVector(colors)\n\u001b[32m     91\u001b[39m o3d.visualization.draw_geometries(\n\u001b[32m     92\u001b[39m     [pc_vis],\n\u001b[32m     93\u001b[39m     window_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBase frame \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     94\u001b[39m     width       = \u001b[32m800\u001b[39m,\n\u001b[32m     95\u001b[39m     height      = \u001b[32m600\u001b[39m\n\u001b[32m     96\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "\n",
    "# PARAMETERS\n",
    "start_frame = 0\n",
    "end_frame   = 93\n",
    "threshold   = 1.2\n",
    "\n",
    "# HELPERS\n",
    "def quat2rot(q):\n",
    "    qw, qx, qy, qz = q['w_val'], q['x_val'], q['y_val'], q['z_val']\n",
    "    return np.array([\n",
    "        [1-2*qy*qy-2*qz*qz,   2*qx*qy-2*qz*qw,     2*qx*qz+2*qy*qw],\n",
    "        [2*qx*qy+2*qz*qw,     1-2*qx*qx-2*qz*qz,   2*qy*qz-2*qx*qw],\n",
    "        [2*qx*qz-2*qy*qw,     2*qy*qz+2*qx*qw,     1-2*qx*qx-2*qy*qy]\n",
    "    ])\n",
    "\n",
    "def get_pose(state):\n",
    "    p = state['kinematics_estimated']['position']\n",
    "    o = state['kinematics_estimated']['orientation']\n",
    "    pos = np.array([p['x_val'], p['y_val'], p['z_val']], dtype=float)\n",
    "    return pos, quat2rot(o)\n",
    "\n",
    "# VIS SETUP (optional – you can swap for video writer or offscreen)\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(visible=False, width=800, height=600)\n",
    "pc_vis = o3d.geometry.PointCloud()\n",
    "vis.add_geometry(pc_vis)\n",
    "ctr = vis.get_view_control()\n",
    "ctr.set_front([0.5, -0.5, -1.0])\n",
    "ctr.set_lookat([0,0,0])\n",
    "ctr.set_up([0,0,1])\n",
    "ctr.set_zoom(0.5)\n",
    "\n",
    "# DOUBLE–LOOP\n",
    "for idx in range(start_frame, end_frame + 1):\n",
    "    plyf = f\"track_1/{idx}.ply\"\n",
    "    carf = f\"track_1/{idx}_car_state.json\"\n",
    "    collf= f\"track_1/{idx}_collision_info.json\"\n",
    "    if not (os.path.exists(plyf) and os.path.exists(carf) and os.path.exists(collf)):\n",
    "        print(f\"[skip {idx}] missing base files\")\n",
    "        continue\n",
    "\n",
    "    # --- load & transform LiDAR at base frame idx\n",
    "    cloud = o3d.io.read_point_cloud(plyf)\n",
    "    pts   = np.asarray(cloud.points)\n",
    "    car0  = json.load(open(carf))\n",
    "    pos0, R0 = get_pose(car0)\n",
    "    world_pts = (R0 @ pts.T).T + pos0\n",
    "\n",
    "    # --- init “ever” masks\n",
    "    ever_close     = np.zeros(len(world_pts), dtype=bool)\n",
    "    ever_collision = np.zeros(len(world_pts), dtype=bool)\n",
    "\n",
    "    # --- sweep through future frames j = idx+1, idx+2, ...\n",
    "    j = idx + 1\n",
    "    while True:\n",
    "        car_jf  = f\"track_1/{j}_car_state.json\"\n",
    "        coll_jf = f\"track_1/{j}_collision_info.json\"\n",
    "        if not (os.path.exists(car_jf) and os.path.exists(coll_jf)):\n",
    "            print(f\"  [stop inner @{j}] no more future data\")\n",
    "            break\n",
    "\n",
    "        # load future car state & collision\n",
    "        car_j = json.load(open(car_jf))\n",
    "        coll_j = json.load(open(coll_jf))\n",
    "\n",
    "        # update car‐proximity mask\n",
    "        pos_car, _ = get_pose(car_j)\n",
    "        d_car      = np.linalg.norm(world_pts - pos_car, axis=1)\n",
    "        ever_close |= (d_car < threshold)\n",
    "\n",
    "        # update collision mask\n",
    "        cp     = coll_j['position']\n",
    "        pos_col= np.array([cp['x_val'], cp['y_val'], cp['z_val']], dtype=float)\n",
    "        d_col  = np.linalg.norm(world_pts - pos_col, axis=1)\n",
    "        ever_collision |= (d_col < threshold)\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    # --- after inner loop: color & visualize\n",
    "    colors = np.ones((len(world_pts), 3)) * 0.7\n",
    "    colors[ever_collision]                    = [1, 0, 0]  # red\n",
    "    colors[ever_close & ~ever_collision]      = [0, 1, 0]  # green\n",
    "\n",
    "    pc_vis.points = o3d.utility.Vector3dVector(world_pts)\n",
    "    pc_vis.colors = o3d.utility.Vector3dVector(colors)\n",
    "    o3d.visualization.draw_geometries(\n",
    "        [pc_vis],\n",
    "        window_name = f\"Base frame {idx}\",\n",
    "        width       = 800,\n",
    "        height      = 600\n",
    "    )\n",
    "    time.sleep(0.1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcc80ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Open3D WARNING] The number of points is 0 when creating axis-aligned bounding box.\n"
     ]
    }
   ],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import time\n",
    "import imageio\n",
    "import cv2\n",
    "# PARAMETERS\n",
    "start_frame = 0\n",
    "end_frame   = 93\n",
    "threshold   = 1.2\n",
    "output_path = \"output1.mp4\"\n",
    "fps         = 10\n",
    "width, height = 800, 600\n",
    "\n",
    "def quat2rot(q):\n",
    "    qw, qx, qy, qz = q['w_val'], q['x_val'], q['y_val'], q['z_val']\n",
    "    return np.array([\n",
    "        [1-2*qy*qy-2*qz*qz,   2*qx*qy-2*qz*qw,     2*qx*qz+2*qy*qw],\n",
    "        [2*qx*qy+2*qz*qw,     1-2*qx*qx-2*qz*qz,   2*qy*qz-2*qx*qw],\n",
    "        [2*qx*qz-2*qy*qw,     2*qy*qz+2*qx*qw,     1-2*qx*qx-2*qy*qy]\n",
    "    ])\n",
    "\n",
    "def get_pose(state):\n",
    "    p = state['kinematics_estimated']['position']\n",
    "    o = state['kinematics_estimated']['orientation']\n",
    "    pos = np.array([p['x_val'], p['y_val'], p['z_val']], dtype=float)\n",
    "    return pos, quat2rot(o)\n",
    "\n",
    "# OFF-SCREEN OPEN3D SETUP\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(visible=True, width=width, height=height)\n",
    "pc_vis = o3d.geometry.PointCloud()\n",
    "vis.add_geometry(pc_vis)\n",
    "# ctr = vis.get_view_control()\n",
    "# ctr.set_front([0.5, 10, -1.0])\n",
    "# ctr.set_lookat([0, 0, 0])\n",
    "# ctr.set_up([0, 0, 1])\n",
    "# ctr.set_zoom(0.5)\n",
    "\n",
    "# # ─── ImageIO writer (note the macro_block_size tweak) ───────────────────────────\n",
    "# writer = imageio.get_writer(\n",
    "#     output_path,\n",
    "#     fps=fps,\n",
    "#     codec=\"libx264\",\n",
    "#     macro_block_size=1   # <— this disables the automatic 16-pixel padding\n",
    "# )\n",
    "\n",
    "for idx in range(start_frame, end_frame + 1):\n",
    "    plyf   = f\"track_1/{idx}.ply\"\n",
    "    carf   = f\"track_1/{idx}_car_state.json\"\n",
    "    collf  = f\"track_1/{idx}_collision_info.json\"\n",
    "    if not (os.path.exists(plyf) and os.path.exists(carf) and os.path.exists(collf)):\n",
    "        print(f\"[skip {idx}] missing base files\")\n",
    "        continue\n",
    "\n",
    "    cloud = o3d.io.read_point_cloud(plyf)\n",
    "    pts   = np.asarray(cloud.points)\n",
    "    car0  = json.load(open(carf))\n",
    "    pos0, R0 = get_pose(car0)\n",
    "    world_pts = (R0 @ pts.T).T + pos0\n",
    "\n",
    "    ever_close     = np.zeros(len(world_pts), dtype=bool)\n",
    "    ever_collision = np.zeros(len(world_pts), dtype=bool)\n",
    "\n",
    "    j =  1\n",
    "    while True:\n",
    "        car_jf  = f\"track_1/{j}_car_state.json\"\n",
    "        coll_jf = f\"track_1/{j}_collision_info.json\"\n",
    "        if not (os.path.exists(car_jf) and os.path.exists(coll_jf)):\n",
    "            break\n",
    "\n",
    "        car_j  = json.load(open(car_jf))\n",
    "        coll_j = json.load(open(coll_jf))\n",
    "\n",
    "        pos_car, _ = get_pose(car_j)\n",
    "        d_car      = np.linalg.norm(world_pts - pos_car, axis=1)\n",
    "        ever_close |= (d_car < (threshold-0.3))\n",
    "        if coll_j['has_collided']:\n",
    "            cp      = coll_j['position']\n",
    "            pos_col = np.array([cp['x_val'], cp['y_val'], cp['z_val']], dtype=float)\n",
    "            d_col   = np.linalg.norm(world_pts - pos_col, axis=1)\n",
    "            ever_collision |= (d_col < (threshold+0.3))\n",
    "\n",
    "        j += 1\n",
    "\n",
    "    colors = np.ones((len(world_pts), 3)) * 0.7\n",
    "    colors[ever_collision]               = [1, 0, 0]\n",
    "    colors[ever_close & ~ever_collision] = [0, 1, 0]\n",
    "\n",
    "    pc_vis.points = o3d.utility.Vector3dVector(world_pts)\n",
    "    pc_vis.colors = o3d.utility.Vector3dVector(colors)\n",
    "    o3d.visualization.draw_geometries(\n",
    "        [pc_vis],\n",
    "        window_name = f\"Base frame {idx}\",\n",
    "        width       = 800,\n",
    "        height      = 600\n",
    "    )\n",
    "    # capture image from visualizer\n",
    "    # img = np.asarray(vis.capture_screen_float_buffer(do_render=True))\n",
    "    # img = np.asarray(vis.capture_screen_float_buffer(do_render=True))\n",
    "    # cv2.imshow(\"img\", img)\n",
    "    # img_uint8 = (img * 255).astype(np.uint8)\n",
    "    # writer.append_data(img_uint8)\n",
    "    # print(f\"Added frame {idx} to video.\")\n",
    "    time.sleep(0.1)\n",
    "\n",
    "# writer.close()\n",
    "# vis.destroy_window()\n",
    "# print(f\"Video saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lidarAIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
