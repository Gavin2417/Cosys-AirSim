C:\Users\gavin\.conda\envs\airEnv\Lib\site-packages\gymnasium\envs\registration.py:481: UserWarning: WARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes']
  logger.warn(
Connected!
Client Ver:3 (Min Req: 3), Server Ver:3 (Min Req: 3)

Using cuda device
Logging to ./tb_logs/ppo_airsim_car_run_1724654819.5675259_1
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 10.1     |
|    ep_rew_mean     | 7.88     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 1        |
|    time_elapsed    | 2998     |
|    total_timesteps | 2048     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 12.3        |
|    ep_rew_mean          | 8.48        |
| time/                   |             |
|    fps                  | 0           |
|    iterations           | 2           |
|    time_elapsed         | 6032        |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011518432 |
|    clip_fraction        | 0.133       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.38       |
|    explained_variance   | 0.000817    |
|    learning_rate        | 0.0003      |
|    loss                 | 51.8        |
|    n_updates            | 10          |
|    policy_gradient_loss | -0.0181     |
|    value_loss           | 98.7        |
-----------------------------------------
Eval num_timesteps=5000, episode_reward=27.89 +/- 15.71
Episode length: 14.40 +/- 7.94
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 14.4        |
|    mean_reward          | 27.9        |
| time/                   |             |
|    total_timesteps      | 5000        |
| train/                  |             |
|    approx_kl            | 0.011078509 |
|    clip_fraction        | 0.161       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.34       |
|    explained_variance   | 0.0248      |
|    learning_rate        | 0.0003      |
|    loss                 | 44.7        |
|    n_updates            | 20          |
|    policy_gradient_loss | -0.0196     |
|    value_loss           | 140         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.5      |
|    ep_rew_mean     | 3.63     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 3        |
|    time_elapsed    | 9193     |
|    total_timesteps | 6144     |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 18.5        |
|    ep_rew_mean          | 2           |
| time/                   |             |
|    fps                  | 0           |
|    iterations           | 4           |
|    time_elapsed         | 12256       |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.013359824 |
|    clip_fraction        | 0.158       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.3        |
|    explained_variance   | 0.0439      |
|    learning_rate        | 0.0003      |
|    loss                 | 64.5        |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.0233     |
|    value_loss           | 146         |
-----------------------------------------
Eval num_timesteps=10000, episode_reward=47.49 +/- 20.97
Episode length: 7.00 +/- 1.10
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 7           |
|    mean_reward          | 47.5        |
| time/                   |             |
|    total_timesteps      | 10000       |
| train/                  |             |
|    approx_kl            | 0.011302759 |
|    clip_fraction        | 0.121       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.26       |
|    explained_variance   | 0.0557      |
|    learning_rate        | 0.0003      |
|    loss                 | 72.4        |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.0168     |
|    value_loss           | 167         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 17.2     |
|    ep_rew_mean     | 21.5     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 5        |
|    time_elapsed    | 15390    |
|    total_timesteps | 10240    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.14        |
|    ep_rew_mean          | 11.9        |
| time/                   |             |
|    fps                  | 0           |
|    iterations           | 6           |
|    time_elapsed         | 18495       |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.010924725 |
|    clip_fraction        | 0.131       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.22       |
|    explained_variance   | 0.0562      |
|    learning_rate        | 0.0003      |
|    loss                 | 66.8        |
|    n_updates            | 50          |
|    policy_gradient_loss | -0.017      |
|    value_loss           | 161         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.43        |
|    ep_rew_mean          | 11.4        |
| time/                   |             |
|    fps                  | 0           |
|    iterations           | 7           |
|    time_elapsed         | 21622       |
|    total_timesteps      | 14336       |
| train/                  |             |
|    approx_kl            | 0.012001909 |
|    clip_fraction        | 0.119       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.16       |
|    explained_variance   | 0.0466      |
|    learning_rate        | 0.0003      |
|    loss                 | 103         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.0154     |
|    value_loss           | 195         |
-----------------------------------------
Eval num_timesteps=15000, episode_reward=58.21 +/- 1.10
Episode length: 7.60 +/- 0.49
----------------------------------------
| eval/                   |            |
|    mean_ep_length       | 7.6        |
|    mean_reward          | 58.2       |
| time/                   |            |
|    total_timesteps      | 15000      |
| train/                  |            |
|    approx_kl            | 0.01247778 |
|    clip_fraction        | 0.146      |
|    clip_range           | 0.2        |
|    entropy_loss         | -1.09      |
|    explained_variance   | 0.0598     |
|    learning_rate        | 0.0003     |
|    loss                 | 121        |
|    n_updates            | 70         |
|    policy_gradient_loss | -0.0189    |
|    value_loss           | 209        |
----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 12.8     |
|    ep_rew_mean     | 13.5     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 8        |
|    time_elapsed    | 24828    |
|    total_timesteps | 16384    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.81        |
|    ep_rew_mean          | 15.8        |
| time/                   |             |
|    fps                  | 0           |
|    iterations           | 9           |
|    time_elapsed         | 28019       |
|    total_timesteps      | 18432       |
| train/                  |             |
|    approx_kl            | 0.016876519 |
|    clip_fraction        | 0.207       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.984      |
|    explained_variance   | 0.0418      |
|    learning_rate        | 0.0003      |
|    loss                 | 61.9        |
|    n_updates            | 80          |
|    policy_gradient_loss | -0.0246     |
|    value_loss           | 184         |
-----------------------------------------
Eval num_timesteps=20000, episode_reward=58.99 +/- 0.42
Episode length: 8.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 8           |
|    mean_reward          | 59          |
| time/                   |             |
|    total_timesteps      | 20000       |
| train/                  |             |
|    approx_kl            | 0.014201479 |
|    clip_fraction        | 0.165       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.909      |
|    explained_variance   | 0.0512      |
|    learning_rate        | 0.0003      |
|    loss                 | 92.7        |
|    n_updates            | 90          |
|    policy_gradient_loss | -0.0235     |
|    value_loss           | 180         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 9.7      |
|    ep_rew_mean     | 35.4     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 10       |
|    time_elapsed    | 31248    |
|    total_timesteps | 20480    |
---------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 9.15        |
|    ep_rew_mean          | 35.1        |
| time/                   |             |
|    fps                  | 0           |
|    iterations           | 11          |
|    time_elapsed         | 34422       |
|    total_timesteps      | 22528       |
| train/                  |             |
|    approx_kl            | 0.012218555 |
|    clip_fraction        | 0.124       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.83       |
|    explained_variance   | 0.107       |
|    learning_rate        | 0.0003      |
|    loss                 | 62.1        |
|    n_updates            | 100         |
|    policy_gradient_loss | -0.0211     |
|    value_loss           | 146         |
-----------------------------------------
-----------------------------------------
| rollout/                |             |
|    ep_len_mean          | 8.42        |
|    ep_rew_mean          | 35          |
| time/                   |             |
|    fps                  | 0           |
|    iterations           | 12          |
|    time_elapsed         | 37577       |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.011055995 |
|    clip_fraction        | 0.114       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.718      |
|    explained_variance   | 0.138       |
|    learning_rate        | 0.0003      |
|    loss                 | 105         |
|    n_updates            | 110         |
|    policy_gradient_loss | -0.019      |
|    value_loss           | 151         |
-----------------------------------------
Eval num_timesteps=25000, episode_reward=59.29 +/- 1.51
Episode length: 8.00 +/- 0.00
-----------------------------------------
| eval/                   |             |
|    mean_ep_length       | 8           |
|    mean_reward          | 59.3        |
| time/                   |             |
|    total_timesteps      | 25000       |
| train/                  |             |
|    approx_kl            | 0.009765685 |
|    clip_fraction        | 0.108       |
|    clip_range           | 0.2         |
|    entropy_loss         | -0.664      |
|    explained_variance   | 0.219       |
|    learning_rate        | 0.0003      |
|    loss                 | 64.3        |
|    n_updates            | 120         |
|    policy_gradient_loss | -0.0157     |
|    value_loss           | 120         |
-----------------------------------------
New best mean reward!
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 14.3     |
|    ep_rew_mean     | 33.5     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 13       |
|    time_elapsed    | 42999    |
|    total_timesteps | 26624    |
---------------------------------
----------------------------------------
| rollout/                |            |
|    ep_len_mean          | 1.24       |
|    ep_rew_mean          | -10        |
| time/                   |            |
|    fps                  | 0          |
|    iterations           | 14         |
|    time_elapsed         | 46854      |
|    total_timesteps      | 28672      |
| train/                  |            |
|    approx_kl            | 0.01945616 |
|    clip_fraction        | 0.189      |
|    clip_range           | 0.2        |
|    entropy_loss         | -0.677     |
|    explained_variance   | 0.445      |
|    learning_rate        | 0.0003     |
|    loss                 | 150        |
|    n_updates            | 130        |
|    policy_gradient_loss | 0.0117     |
|    value_loss           | 216        |
----------------------------------------
Eval num_timesteps=30000, episode_reward=49.97 +/- 19.08
Episode length: 7.40 +/- 1.20
------------------------------------------
| eval/                   |              |
|    mean_ep_length       | 7.4          |
|    mean_reward          | 50           |
| time/                   |              |
|    total_timesteps      | 30000        |
| train/                  |              |
|    approx_kl            | 0.0056414716 |
|    clip_fraction        | 0.0625       |
|    clip_range           | 0.2          |
|    entropy_loss         | -0.661       |
|    explained_variance   | 0.211        |
|    learning_rate        | 0.0003       |
|    loss                 | 76.8         |
|    n_updates            | 140          |
|    policy_gradient_loss | -0.0111      |
|    value_loss           | 123          |
------------------------------------------
---------------------------------
| rollout/           |          |
|    ep_len_mean     | 8.95     |
|    ep_rew_mean     | 22.9     |
| time/              |          |
|    fps             | 0        |
|    iterations      | 15       |
|    time_elapsed    | 51481    |
|    total_timesteps | 30720    |
---------------------------------